"""
BootstrapTabularExplainer class and various helper functions for generating local explanations of ML models
given a static dataset of model inputs & outputs instead of direct model access. 
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
from sklearn import preprocessing
from scipy.spatial.distance import euclidean
import scipy.stats as st
from random import sample

plt.rcParams['figure.figsize'] = (10, 5)
plt.rc('font', size=14) 
plt.rc('legend', fontsize=13)


def plot_slice(f, lims, const_values, dim, feat_type='continuous', label=''):
    """
    Visualizes a function along a single input dimension, holding other
    values constant.
    
    Parameters:
        - f (function): function that takes a length-d np.ndarray vector, x, and returns a scalar, f(x)
        - lims (list): limits of the x-axis for plotting (lims[0]: min value; lims[1]: max value)
        - const_values (np.ndarray): length-d vector specifying the values at which other features should be held constant
                        (Note: the value of const_values[dim] doesn't matter since it will be varied anyway.)
        - dim (int): dimension along which to plot f 
    """
    if feat_type=='continuous':
        slice_vals = np.linspace(lims[0], lims[1], 1000)
    elif feat_type=='categorical':
        slice_vals = np.arange(lims[0], lims[1]+1)

    targets = [np.copy(const_values) for x in slice_vals]

    for i in range(len(targets)):
        targets[i][dim] = slice_vals[i]
    
    f_vals = [f(t) for t in targets]
    
    plt.plot(slice_vals, f_vals, linewidth=3)
    plt.xlim(lims)
    plt.xlabel(label)
    plt.ylabel(r'function value')
    

def derivative(f, target, dim):
    """
    Returns the numerical derivative of a scalar function at a target point, 
    with respect to a specified dimension. 
    
    Parameters:
        - f (function): scalar function to differentiate
        - target (np.ndarray): point at which to compute the derivative
        - dim (int): dimension along which to compute the derivative
    """
    h = 1e-6
    target = np.array(target, dtype=float)
    point_right = np.copy(target)
    point_left = np.copy(target)
    point_right[dim] += h
    point_left[dim] -= h
    return (f(point_right) - f(point_left))/(2*h)    
    
def gradient(f, target):
    """
    Returns the gradient of a function at a target point. 
    
    Parameters:
        - f (function): scalar function to differentiate
        - target (np.ndarray): point at which to compute the gradient
    """
    return np.array([derivative(f, target, d) for d in range(len(target))])


def plot_confidence(explanation):
    """
    Plots feature importance scores and their corresponding confidence intervals. 
    (Helper function for visualizing model explanations with BootstrapTabularExplainer.)

    Parameters:
        - explanation: dictionary in the same format as that returned by BootstrapTabularExplainer.explain_instance()
    """
    data_dict = {}
    num_features = len(explanation)
    data_dict['feature'] = ["{} ({})".format(explanation[d]['feature_name'], explanation[d]['feature_value']) for d in range(num_features)]
    data_dict['lower'] = [explanation[d]['lower_confidence'] for d in range(num_features)]
    data_dict['estimate'] = [explanation[d]['importance'] for d in range(num_features)]
    data_dict['upper'] = [explanation[d]['upper_confidence'] for d in range(num_features)]
    dataset = pd.DataFrame(data_dict)

    for lower,estimate,upper,y in zip(dataset['lower'],dataset['estimate'],dataset['upper'],range(len(dataset))):
        plt.plot((lower,upper),(y,y),'o-', color='blue', linewidth=3)
        plt.plot(estimate, y, 'o', color='orange')
    plt.yticks(range(len(dataset)),list(dataset['feature']))
    plt.xlabel('Feature Importance')
    plt.title('Bootstrap Confidence Intervals')
    plt.show()



#################################
#     Bootstrap Explanations    #
#################################


class BootstrapTabularExplainer(object):
    """
    Class for generating local explanations of a machine learning model given a historical dataset of 
    model inputs & outputs, rather than direct model access. Explanations are generated by
    fitting a low-degree polynomial to the points in the dataset closest to the input instance, then
    computing the partial derivatives or function differences of this polynomial w.r.t. each feature. 
    Also provides the option to attach bootstrap confidence intervals to the explanation to quantify
    the uncertainty in the estimated feature importance scores. 

    Note: The current version only supports tabular data for regression & binary classification problems
          It does not yet support multiclass classification or other data modalities (e.g., text, images). 
    """
    
    def __init__(self,
                 data, 
                 model_outputs, 
                 mode="regression",
                 explanation_type="gradient", 
                 deltas=None, 
                 weighted=True,
                 feature_names=None, 
                 categorical_features=[], 
                 categorical_baselines=None, 
                 category_names=None 
                ):
        """
        Initializes the explainer. 

        Parameters:
            - data (np.ndarray): data matrix with dimensions N x d, where N is the number of examples and d is the number of features.       
                    Categorical variables must be encoded as integers (e.g,. using LabelEncoder)
            - model_outputs (np.ndarray): vector of model outputs with length equal to the number of rows in `data`. Should be 
                    probabilities (values in [0,1]) if doing classification. 
            - mode ("regression" or "classification"): the type of model we're trying to explain.
            - explanation_type ("gradient" or "diffs"): which type of explanation to generate for continuous features. 
                    If "gradient", estimates the instantaneous partial derivative w.r.t. each feature. 
                    If "diffs", estimates function differences. This is useful if the model has a lot of "flat" regions where
                    the instantaneous derivative is zero. 
            - deltas: dictionary mapping continuous feature indices to deltas for computing function differences. 
                    Can be specified if explanation_type="diffs". If unspecified, will be set by default to 1/2 
                    the standard deviation of each continuous feature. 
            - weighted (bool): whether to use weighted local regression.
            - feature_names: list of feature names
            - categorical_features: indices of columns in data corresponding to categorical features
            - categorical_baselines: maps from column index of a categorical feature to its baseline/counterfactual category (int)
            - category_names: dictionary where category_names[i][j] is the name of the jth category of the ith feature in the dataset 
        """
    
        
        self.data = data.astype(float)
        self.num_features = data.shape[1]
        self.mode = mode
        
        if self.mode == 'classification':
            # Convert probabilities to log-odds
            preds = np.copy(model_outputs)
            preds[np.where(preds==0)] += 1e-8 # prevents -inf log-odds
            preds[np.where(preds==1)] -= 1e-8 # prevents divide-by-zero errors
            log_odds = np.log(preds/(1-preds))
            self.model_outputs = log_odds
        elif self.mode == 'regression':
            self.model_outputs = model_outputs

        self.explanation_type = explanation_type
        self.deltas = deltas
        self.weighted = weighted
        self.category_names = category_names  
        
        self.query = None
        self.local_model = None 
        self.k = None
        self.neighbors = None
        self.neighbors_labels = None
        
        self.categorical_features = categorical_features
        self.continuous_features = [i for i in range(self.num_features) if i not in self.categorical_features]

        # If the user wants to compute function differences and didn't specify their own deltas, set them 
        # proportional to the standard deviation of each continuous feature. 
        if self.explanation_type=='diffs' and deltas is None:
            self.deltas = {c:np.std(self.data[:,c])/2 for c in self.continuous_features}
        
        # Categories sorted by how popular they are in the dataset
        categories_ranked = {c:None for c in self.categorical_features}
        for c in self.categorical_features:
            categories = np.unique(self.data[:,c])
            category_counts = np.array([np.sum(self.data[:,c] == cat) for cat in categories])
            categories_ranked[c] = categories[np.argsort(-category_counts)]
        
        # If user did not specify baselines for categorical features, set them to the most 
        # frequently occurring categories in the dataset. 
        if categorical_baselines is None:
            self.categorical_baselines = {c:categories_ranked[c][0] for c in self.categorical_features}
        else:
            self.categorical_baselines = categorical_baselines 
        self.original_baselines = self.categorical_baselines.copy()
        
        # Alternative baselines to use in case the query category matches the original baseline. 
        self.baseline_alternatives = {c:None for c in self.categorical_features}
        for c in self.categorical_features:
            if c not in self.categorical_baselines:
                self.categorical_baselines[c] = categories_ranked[c][0]
            if  self.categorical_baselines[c] == categories_ranked[c][0]:
                self.baseline_alternatives[c] = categories_ranked[c][1]
            else:
                self.baseline_alternatives[c] = categories_ranked[c][0]

        # Set up the one-hot encoder. Drop the baseline categories to prevent multicollinearity issues. 
        if len(self.categorical_features) > 0:
            baselines = np.array([self.categorical_baselines[c] for c in self.categorical_features])
            enc_onehot = preprocessing.OneHotEncoder(drop=baselines)
            enc_onehot.fit(self.data[:,self.categorical_features])
            self.enc_onehot = enc_onehot
        
        if feature_names is None:
            feature_names = [str(i) for i in range(self.num_features)]
        self.feature_names = list(feature_names)
        
        # Store the feature means & standard deviations to perform standardization later
        self.data_mean = np.mean(data[:,self.continuous_features], axis=0)
        self.data_std = np.std(data[:,self.continuous_features], axis=0)
        
        
    def update_onehot(self):
        """
        Updates the OneHotEncoder with the currently stored categorical_baselines. 
        """
        if len(self.categorical_features) > 0:
            baselines = np.array([self.categorical_baselines[c] for c in self.categorical_features])
            enc_onehot = preprocessing.OneHotEncoder(drop=baselines)
            enc_onehot.fit(self.data[:,self.categorical_features])
            self.enc_onehot = enc_onehot
    
    def distance(self, p, q):
        """
        Computes the Euclidean distance between the continuous features of p and q.

        Parameters:
            - p (np.ndarray) 
            - q (np.ndarray)
        """
        assert len(p)==len(q)
        continuous_contribution = np.sum((p[self.continuous_features]-q[self.continuous_features])**2)
        return np.sqrt(continuous_contribution)
           
        
    def set_neighbors(self, num_neighbors):
        """
        Identifies the num_neighbors closest sampled points to the query point. 

        Parameters:
            - num_neighbors (int): number of neighbors 
        """
 
        # Standardize the features (subtract mean & divide by std. deviation)
        data_standardized = np.copy(self.data)
        data_standardized[:,self.continuous_features] = (self.data[:,self.continuous_features] - self.data_mean) / self.data_std

        query_standardized = np.copy(self.query)
        query_standardized[self.continuous_features] = (self.query[self.continuous_features] - self.data_mean) / self.data_std

        # Sort points in the dataset by euclidean distance between continuous features. 
        dists = np.array([self.distance(query_standardized, s) for s in data_standardized])
        closest_idx = np.argsort(dists)
        data_sorted = data_standardized[closest_idx]

        # Try to get roughly equal representation between the baseline category and the query category. 
        num_categorical = len(self.categorical_features)
        num_per_category = int(num_neighbors/(2*num_categorical)) if num_categorical > 0 else 0 
        neighbor_idx = []
        for c in self.categorical_features:
            base_category = self.categorical_baselines[c]
            assert base_category != self.query[c]
            query_idx = closest_idx[np.where(data_sorted[:,c] == self.query[c])]
            base_idx = closest_idx[np.where(data_sorted[:,c] == base_category)]

            for i in range(num_per_category):
                if i < len(query_idx) and query_idx[i] not in neighbor_idx: neighbor_idx.append(query_idx[i]) 
                if i < len(base_idx) and base_idx[i] not in neighbor_idx: neighbor_idx.append(base_idx[i]) 

        # Add extra points if we're still short of num_neighbors
        closest_idx = [i for i in closest_idx if i not in neighbor_idx]
        neighbor_idx.extend(closest_idx[0:num_neighbors-len(neighbor_idx)])
        
        neighbors_in = self.data[neighbor_idx]
        neighbors_out = self.model_outputs[neighbor_idx]
        
        self.neighbors = neighbors_in
        self.neighbors_labels = neighbors_out

    
    def preprocess_data(self, X, k):
        """
        Preprocesses the data by performing a polynomial transformation of
        continuous features and one-hot encoding of categorical features. 
        Also includes interaction terms between continuous & categorical
        dummy variables. 

        Parameters:
            - X (np.ndarray): data matrix to process
            - k (int): degree of the local polynomial

        Returns:
            An np.ndarray matrix with dimensions N x p where N is the number
            of rows in X and p is the total number of model parameters. 
        """
        poly_features = preprocessing.PolynomialFeatures(degree=k)
        
        if len(self.categorical_features)==0: # only continuous features
            return poly_features.fit_transform(X)
        else:
            # Transform continuous features to polynomial features
            X_poly = poly_features.fit_transform(X[:,self.continuous_features])
            
            # Perform one-hot encoding of categorical features
            X_onehot = self.enc_onehot.transform(X[:,self.categorical_features]).toarray()
            
            # Add interaction terms between continuous and categorical dummy variables.
            # Include higher-order terms for continuous vars but not for dummy vars. 
            poly_interact = preprocessing.PolynomialFeatures(degree=k-1, include_bias=False)
            X_poly_interact = poly_interact.fit_transform(X[:,self.continuous_features])
            num_poly = X_poly_interact.shape[1]
            X_poly_onehot = np.zeros((X.shape[0], num_poly*X_onehot.shape[1]))
            
            for i in range(X_onehot.shape[1]):
                X_poly_onehot[:, i*num_poly:(i+1)*num_poly] = np.multiply(X_poly_interact, np.transpose([X_onehot[:,i]]))

            # Put everything together in a data matrix
            X_all = np.concatenate((X_poly, X_onehot, X_poly_onehot), axis=1)
            
            return X_all 
        
    def fit_local_poly(self, k):
        """
        Fits a degree-k polynomial to the data. 

        Parameters:
            k (int): degree of the local polynomial
        """
        X = self.preprocess_data(self.neighbors, k)
        self.k = k
        
        if self.weighted:
            data_standardized = np.copy(self.neighbors)
            data_standardized[:,self.continuous_features] = (self.neighbors[:,self.continuous_features] - self.data_mean) / self.data_std
            
            query_standardized = np.copy(self.query)
            query_standardized[self.continuous_features] = (self.query[self.continuous_features] - self.data_mean) / self.data_std
            
            dists = np.array([self.distance(query_standardized, s) for s in data_standardized])
            weights = 1-(dists - np.min(dists)) / (np.max(dists)- np.min(dists)) 
            model = sm.WLS(self.neighbors_labels, X, weights)
        else: 
            model = sm.OLS(self.neighbors_labels, X) 
        
        fitted = model.fit() 
        self.local_model = fitted
 
    
    def local_predict(self, target):
        """
        Returns the prediction of the currently stored local polynomial model
        at specified point(s). 

        Parameters:
            - target (np.ndarray): point(s) at which to compute the function value

        Returns:
            A scalar if `target` represents a single point, otherwise an np.ndarray vector
            whose length is equal to the number of target points. 
        """
        assert self.local_model is not None
        
        targ = np.copy(target)
        if len(target.shape)==1:
            targ = target.reshape(1, -1)
        
        X = self.preprocess_data(targ, self.k)
        pred = self.local_model.predict(X)
        
        if len(pred)==1:
            pred = pred.item()
            
        # Convert from log-odds to probabilities if doing classification
        if self.mode=='classification':
            if pred > 50: pred = 1.
            elif pred < -50: pred = 0.
            else: pred = 1/(1+np.exp(-pred))
            
        return pred
    
    
    def get_continuous_diff(self, f, query, dim):
        """
        Returns the function difference for a continuous feature. 

        Parameters:
            - f (function): function for computing differences
            - query (np.ndarray): point around which to compute differences
            - dim (int): column index of the continuous feature

        Returns:
            A scalar equal to f(x_+) - f(x_-) where x_+ equals 
            query[dim]+delta/2 and x_- equals query[dim]-delta/2.
        """
        delta = self.deltas[dim]
        query_right = np.copy(query)
        query_left = np.copy(query)
        query_right[dim] += delta/2
        query_left[dim] -= delta/2
        return f(query_right) - f(query_left)
    
    def get_categorical_diffs(self, f, query, dim):
        """
        Returns function differences (all counterfactuals relative to 
        the currently stored baseline) for a categorical feature. 

        Parameters:
            - f (function): function for computing differences
            - query(np.ndarray): point around which to compute differences
            - dim (int): column index of the categorical feature 

        Returns:
            A list where each entry represents f(alternative)-f(baseline) where 
            `baseline` equals `query` with the `dim` entry equal to the baseline category,
            and `alternative` is the same but with the `dim` entry equal to some other category.
        """
        assert dim in self.categorical_features 

        col_to_count = {self.categorical_features[i]:i for i in range(len(self.categorical_features))}
        query_baseline = np.copy(query)
        base_category = self.categorical_baselines[dim]
        query_baseline[dim] = base_category
        diffs = []

        for c in self.enc_onehot.categories_[col_to_count[dim]]:
            if c != base_category:
                query_c = np.copy(query)
                query_c[dim] = c
                diff = f(query_c) - f(query_baseline)
                diffs.append(diff)
        return diffs
    
    def get_gradient(self, f, query):
        """
        Returns the gradient of a function at a specific point.
        For categorical variables, returns function differences
        instead of instantaneous partial derivatives. 

        Parameters:
            - f (function): function to differentiate 
            - query (np.ndarray): point at which to compute the gradient

        Returns:
            A list where the `i`th entry is either:
                - a list of function differences if `i` is a categorical feature
                - a length-1 list containing the partial derivative of `f` w.r.t. 
                  the `i`th feature if `i` is a continuous feature. 
        """
        grad_estimate = []
        
        for d in range(self.num_features):
            if d in self.categorical_features:
                # Compute function differences for categorical features
                grad_estimate.append(self.get_categorical_diffs(f, query, d))
            else:
                # Compute instantaneous derivative w.r.t. continuous features
                grad_estimate.append([derivative(f, query, d)])
    
        return grad_estimate
    
    
    def get_diffs(self, f, query):
        """
        Returns function differences around a specific point. 

        Parameters:
            - f (function): function for computing differences
            - query (np.ndarray): point around which to compute differences
        
        Returns:
            A list where the `i`th entry is a list of function differences w.r.t.
            the `i`th feature. 
        """
        diffs = []
        for d in range(self.num_features):
            if d in self.categorical_features:
                diffs.append(self.get_categorical_diffs(f, query, d))
            else:
                diffs.append([self.get_continuous_diff(f, query, d)])  
        return diffs

    
    def get_point_estimate(self, query, m, k):
        """
        Computes & returns the estimated feature importance scores. 

        Parameters:
            - query (np.ndarray): point to explain
            - m (int): number of nearest points to use when constructing the explanation
            - k (int): degree of the local polynomial 

        Returns:
            A list where the `i`th entry is a list of feature importance scores for the
            `i`th feature. If a feature is categorical, it potentially has multiple importance
            scores corresponding to all the counterfactuals. 
        """
        self.categorical_baselines = self.original_baselines.copy() 
        self.query = query.astype(float)

        # Change baselines if any of them collide w/ the query categories
        for c in self.categorical_features:
            if self.query[c] == self.categorical_baselines[c]:
                self.categorical_baselines[c] = self.baseline_alternatives[c]
        self.update_onehot() # update one-hot encoder w/ the new baselines 
    
        self.set_neighbors(m)
        self.fit_local_poly(k)
        
        if self.explanation_type == 'gradient':
            estimate = self.get_gradient(self.local_predict, query)
        elif self.explanation_type == 'diffs':
            estimate = self.get_diffs(self.local_predict, query)

        return estimate

    
    def explain_instance(self, query, m, k, verbose=False, with_confidence=False, boot_ratio=0.9, boot_method='percentile', num_bootstrap=1000, conf_level=0.95):
        """
        Builds a local model explanation around an input instance. 

        Parameters:
            - query (np.ndarray): point to explain
            - num_neighbors (int): number of nearest neighbors to use when constructing the explanation
            - k (int): degree of the local polynomial 
            - verbose (bool, optional): if True, print a detailed description of the explanation (with a plot if with_confidence=True)
            - with_confidence (bool, optional): if True, print & display bootstrap confidence intervals alongside the explanation
            - boot_ratio (float, optional): determines the size of each bootstrap sample. The size is set equal to int(boot_ratio*num_neighbors)
            - boot_method ("percentile" or "basic", optional): which bootstrap method to use. The "basic" bootstrap is also known as the   
                    "reverse percentile" bootstrap.
            - num_bootstrap (int, option): number of bootstrap samples to draw when estimating the sampling distribution. 
            - conf_level (float, optional): confidence level of the bootstrap interval. 

        Returns:
            A list of dictionaries, `explanation`, where explanation[d] contains information about
            the explanation w.r.t. feature `d`. For example, explanation[d]['importance'] gives the
            importance score of feature `d`. If with_confidence=True, then explanation[d]['lower_confidence'] 
            and explanation[d]['upper_confidence'] give the upper and lower bounds of the bootstrap confidence
            interval for feature `d`. 

            Note: For categorical features, we take the importance score to be f(query) - f(baseline), where `baseline` is
                  equal to `query` with the category switched to the baseline category.
                  If *all* counterfactuals are desired, take a look at the bootstrap_CI() function which returns more information.
        """

        pt_estimates = self.get_point_estimate(query, m, k)

        if with_confidence:
            #assert m_boot is not None
            m_boot = int(len(self.neighbors_labels)*boot_ratio)
            bootstrap = self.bootstrap_CI(query, num_neighbors=m, k=k, boot_size=m_boot, boot_method=boot_method, num_bootstrap=num_bootstrap, conf_level=conf_level)
            #pt_estimates = [bootstrap[d][0] for d in range(self.num_features)]
            lower_bounds = [bootstrap[d][1] for d in range(self.num_features)]
            upper_bounds = [bootstrap[d][2] for d in range(self.num_features)]
            

        # Filter pt_estimates, lower_bounds and upper_bounds for categorical features to include
        # only function differences relevant to the query category
        explanation = [{} for d in range(self.num_features)]
        for d in range(self.num_features):
            explanation[d]['feature_name'] = self.feature_names[d]
            if d in self.continuous_features:
                explanation[d]['feature_type'] = 'continuous'
                explanation[d]['feature_value'] = query[d]
                explanation[d]['importance'] = pt_estimates[d][0]

                if with_confidence:
                    explanation[d]['lower_confidence'] = lower_bounds[d][0]
                    explanation[d]['upper_confidence'] = upper_bounds[d][0]
            else:
                explanation[d]['feature_type'] = 'categorical'
                base_category = self.categorical_baselines[d]
                
                if self.category_names is not None: 
                    base_name = self.category_names[d][int(base_category)]
                    explanation[d]['feature_value'] = self.category_names[d][int(query[d])]
                else:
                    base_name = int(base_category)
                    explanation[d]['feature_value'] = int(query[d])

                scores = np.array(pt_estimates[d])
                col_to_count = {self.categorical_features[i]:i for i in range(len(self.categorical_features))}
                categories = list(self.enc_onehot.categories_[col_to_count[d]])
                categories.remove(base_category)

                # baseline --> query category function difference
                loc = categories.index(int(query[d]))
                importance = scores[loc]
                
                if with_confidence:
                    explanation[d]['lower_confidence'] = lower_bounds[d][loc]
                    explanation[d]['upper_confidence'] = upper_bounds[d][loc]

                explanation[d]['baseline_category'] = base_name
                explanation[d]['importance'] = importance


        if verbose:
            print("\nLocal model explanation (# neighbors={}, polynomial degree={}, # parameters={}, R-squared={:.5f}): ".format(m,k,len(self.local_model.params),self.local_model.rsquared))
            if with_confidence: print("Bootstrap method: ", boot_method)
            print("\n")
            for d in range(self.num_features):
                print("Feature:  {} = {}".format(explanation[d]['feature_name'], explanation[d]['feature_value']))
                if d in self.categorical_features: print("Baseline category:  {}".format(explanation[d]['baseline_category']))
                print("Importance score:  {:.5f}".format(explanation[d]['importance']))

                if with_confidence:
                    print("Bootstrap confidence interval:  [{:.5f}, {:.5f}],  Width={:.5f}".format(explanation[d]['lower_confidence'], explanation[d]['upper_confidence'], explanation[d]['upper_confidence']-explanation[d]['lower_confidence']))
                print("\n")
        
            # Plot the confidence intervals
            if with_confidence: plot_confidence(explanation)
    
        return explanation 
    
    
    def plot_neighbors(self, dim):
        """
        Produces a scatter plot of the nearest neighbors to the query point,
        projected along a specified dimension. 

        Parameters:
            - dim (int): dimension along which to visualize the neighbors 
        """
        # Convert from log-odds to probabilities if doing classification
        if self.mode=='classification':
            neighbor_probs = 1/(1+np.exp(-self.neighbors_labels))
            plt.scatter(self.neighbors[:,dim], neighbor_probs, color='red', marker='.')
        elif self.mode=='regression':
            plt.scatter(self.neighbors[:,dim], self.neighbors_labels, color='red', marker='.')
        
    def plot_local_slice(self, dim):
        """
        Plots the currently stored local model along a specified dimension. 

        Parameters:
            - dim (int): dimension along which to visualize the function
        """
        assert self.query is not None
        feat_type = 'continuous' if dim in self.continuous_features else 'categorical'
        plot_slice(self.local_predict, lims=[np.min(self.data[:,dim]), np.max(self.data[:,dim])], const_values=self.query, dim=dim, label=self.feature_names[dim], feat_type=feat_type)
        
        
    def visualize_CI(self, CI_type, ground_truth, const_values, dim, num_neighbors, k, conf_level=0.95, boot_size=None):
        """
        Plots point estimates & confidence intervals along a single dimension.

        Parameters:
            - CI_type (string): which confidence interval to construct ('bootstrap' or 'theoretical')
            - ground_truth (function): the ground truth model for comparison purposes 
            - const_values (np.ndarray): vector specifying the values at which other features should be held constant
                        (note: the value of const_values[dim] doesn't matter)
            - dim (int): dimension along which to plot
            - num_neighbors (int): number of nearest neighbors to use when constructing the explanation
            - k (int): degree of the local polynomial model
            - conf_level (float, optional): confidence level 
            - boot_size (int, optional): size of each bootstrap sample (< num_neighbors); must be specified if CI_type=='bootstrap'
        """

        if CI_type=='bootstrap': assert boot_size is not None and boot_size < num_neighbors 
        
        lims = [np.min(self.data[:,dim]), np.max(self.data[:,dim])]
        
        if dim in self.continuous_features:
            slice_vals = np.linspace(lims[0], lims[1], 100)
        else:
            slice_vals = np.arange(lims[0], lims[1]+1)

        targets = [np.copy(const_values) for x in slice_vals]
        results = []
        
        for i in range(len(targets)):
            print(i, end='\r')
            targets[i][dim] = slice_vals[i]
            if CI_type=='bootstrap':
                results.append(self.bootstrap_CI(targets[i], num_neighbors, k, boot_size, boot_method)[dim])
            elif CI_type=='theoretical':
                results.append(self.theoretical_CI(targets[i], num_neighbors, k)[dim])

        # Produce one plot for each regressor associated with this feature (could be serveral for one-hot encoded categorical features)
        for i in range(len(results[0][0])):

            if self.explanation_type=='gradient':
                df_vals = [self.get_gradient(ground_truth, t)[dim][i] for t in targets]
            elif self.explanation_type=='diffs':
                df_vals = [self.get_diffs(ground_truth, t)[dim][i] for t in targets]

            pt_estimates = [r[0][i] for r in results]
            CI_lower_bounds = [r[1][i] for r in results]
            CI_upper_bounds = [r[2][i] for r in results]
            
            plt.plot(slice_vals, df_vals, linewidth=3)
            plt.plot(slice_vals, pt_estimates, color='red', linewidth=3)  
            plt.fill_between(slice_vals, CI_lower_bounds, CI_upper_bounds, alpha=0.4, facecolor='red')   
            plt.xlim(lims)
            plt.xlabel(self.feature_names[dim])
            
            if dim in self.categorical_features or self.explanation_type=='diffs':
                plt.ylabel("$\Delta f$")
                plt.legend(["true differences", "estimated differences", "{}% CI".format(int(conf_level*100))])
            else:
                plt.ylabel("$\partial f / \partial$ {}".format(self.feature_names[dim]))
                plt.legend(["true derivative", "estimated derivative", "{}% CI".format(int(conf_level*100))])
            plt.show()

    
    def bootstrap_sampling_distribution(self, query, num_neighbors, k, boot_size, num_bootstrap=1000, conf_level=0.95):
        """
        Computes the bootstrap sampling distribution from the neighbhorhood around a point.  

        Parameters:
            - query (np.ndarray): point to explain
            - num_neighbors (int): number of nearest neighbors to use when constructing the explanation
            - k (int): degree of the local polynomial
            - boot_size (int): size of each bootstrap sample
            - num_bootstrap (int, optional): number of bootstrap samples to take
            - conf_level (float, optional): confidence level 

        Returns:
            A dictionary, `estimates`, where estimates[d] is an np.ndarray matrix with shape (num_bootstrap, 1) if
            the `d`th feature is continuous and (num_bootstrap, C-1) otherwise, where `C` is the number of categories. 
            Each row represents the estimated feature importance scores obtained from a particular bootstrap sample. 
            For categorical features, note that there is an importance score for *each counterfactual*, which is why
            there are C-1 of them.
        """
        self.query = query.astype(float)
        self.set_neighbors(num_neighbors)
        neighbor_samples = [(self.neighbors[i], self.neighbors_labels[i]) for i in range(len(self.neighbors_labels))]
        
        estimates = {d:[] for d in range(self.num_features)}
        
        for t in range(num_bootstrap):
            # Generate bootstrap samples (without replacement)
            boot_samples = sample(neighbor_samples, boot_size) # 2nd argument is the sample size (should be < m)
            boot_samples_in = np.array([s[0] for s in boot_samples])
            boot_samples_out = np.array([s[1] for s in boot_samples])
            
            self.neighbors = boot_samples_in
            self.neighbors_labels = boot_samples_out
        
            self.fit_local_poly(k)
            
            if self.explanation_type=='gradient':
                est = self.get_gradient(self.local_predict, query)
            elif self.explanation_type=='diffs':
                est = self.get_diffs(self.local_predict, query)
                
            for d in range(self.num_features): estimates[d].append(est[d])
                    
        estimates = {d:np.array(estimates[d]) for d in range(self.num_features)}
        return estimates
    

    def bootstrap_CI(self, query, num_neighbors, k, boot_size, boot_method='percentile', conf_level=0.95, num_bootstrap=1000):
        """
        Returns bootstrap confidence intervals of feature importances. 

        Parameters:
            - query (np.ndarray): point to explain  
            - num_neighbors (int): number of nearest neighbors to use when constructing the explanation
            - k (int): degree of the local polynomial
            - boot_size (int): size of each bootstrap sample
            - conf_level (float, optional): confidence level 

        Returns:
            A dictionary, `results`, where results[d] is a tuple of the form (pt, CI_lower, CI_upper) where:
                - `pt` is an np.ndarray vector of estimated feature importance scores for the `d`th feature
                - `CI_lower` is an np.ndarray vector (same length as `pt`) of bootstrap confidence interval lower bounds
                - `CI_upper` is an np.ndarray vector (same length as `pt`) of bootstrap confidence interval upper bounds
           
            Note: If the `d`th feature is continuous, all three of the above will have 1 element. They have length > 1 only if
                  the `d`th feature is categorical and has multiple counterfactuals. 
        """
        assert boot_size is not None and boot_size < num_neighbors 

        results = {d:None for d in range(self.num_features)}
        
        pt_estimate = self.get_point_estimate(query, num_neighbors, k)
        bootstrap_grads = self.bootstrap_sampling_distribution(query, num_neighbors, k, boot_size, num_bootstrap, conf_level)
        
        idx_lower = int(num_bootstrap/2 - conf_level*num_bootstrap/2) 
        idx_upper = int(num_bootstrap/2 + conf_level*num_bootstrap/2) 
        
        for d in range(self.num_features):
            df_sorted = np.sort(bootstrap_grads[d], axis=0)
            CI_lower = df_sorted[idx_lower,:] # alpha/2 percentile
            CI_upper = df_sorted[idx_upper,:] # 1-alpha/2 percentile
            pt = np.array(pt_estimate[d])

            if boot_method=='percentile':
                results[d] = (pt, CI_lower, CI_upper)
            elif boot_method=='basic':
                results[d] = (pt, 2*pt-CI_upper, 2*pt-CI_lower)
        
        return results



    def onehot_transform(self, ab_data):
        """
        Helper function for theoretical_CI(). 
        """
        X_onehot = self.enc_onehot.transform(ab_data).toarray()
        A1 = X_onehot[:,0]
        A2 = X_onehot[:,1]
        B1 = X_onehot[:,2]
        B2 = X_onehot[:,3]
        
        return A1, A2, B1, B2

    def theoretical_CI(self, query, num_neighbors, k, conf_level=0.95):
        """
        Returns theoretical confidence intervals of feature importances, under the
        assumption that the sampling distribution is Gaussian and centered around
        the true importance value.  

        Note: The current implementation is specific to our simulations and does not 
              generalize to other settings. 

        Parameters:
            - query (np.ndarray): point to explain
            - num_neighbors (int): number of nearest neighbors to use when constructing the explanation
            - k (int): degree of the local polynomial
            - conf_level (float, optional): confidence level 

        Returns:
            Same format as bootstrap_CI(). 
        """
        z_score = st.norm.ppf((1+conf_level)/2)
        results = {d:None for d in range(self.num_features)}

        pt_estimate = self.get_point_estimate(query, num_neighbors, k)
        cov_matrix = self.local_model.cov_params() 

        p = preprocessing.PolynomialFeatures(degree=k)
        Xp = p.fit_transform(self.neighbors[:,0:2])
        pows = p.powers_
        
        # Degree k-1 polynomial for categorical-continuous interaction terms
        p_int = preprocessing.PolynomialFeatures(degree=k-1, include_bias=False)
        Xpi = p_int.fit_transform(self.neighbors[:,0:2])
        pows_int = p_int.powers_
        
        # Convert a,b to one-hot encoding
        query_ab = query[2:].reshape(1, -1)
        A1, A2, B1, B2 = self.onehot_transform(query_ab)
        
        # Continuous features (u, v)
        for d in [0,1]:
            v = np.zeros((len(cov_matrix),))
            
            # Continuous-only terms
            # Reduce exponents of relevant terms by 1
            pows_df = np.copy(pows)
            pows_df[:,d] -= 1
            pows_df = np.clip(pows_df, a_min=0, a_max=None)
            v[0:len(pows_df)] = pows[:,d]*np.prod(query[0:2]**pows_df, axis=1)
            
            # Categorical-continuous interaction terms
            pows_int_df = np.copy(pows_int)
            pows_int_df[:,d] -= 1
            pows_int_df = np.clip(pows_int_df, a_min=0, a_max=None)
            continuous_component = pows_int[:,d]*np.prod(query[0:2]**pows_int_df, axis=1)
            
            v[len(pows)+4:len(pows)+4+len(pows_int)] = A1*continuous_component
            v[len(pows)+4+len(pows_int):len(pows)+4+2*len(pows_int)] = A2*continuous_component
            v[len(pows)+4+2*len(pows_int):len(pows)+4+3*len(pows_int)] = B1*continuous_component
            v[len(pows)+4+3*len(pows_int):] = B2*continuous_component
            
            # Compute std. error of estimate and confidence interval
            df_stderr = np.sqrt(v.T.dot(cov_matrix.dot(v)))
            CI_lower = pt_estimate[d] - z_score*df_stderr
            CI_upper = pt_estimate[d] + z_score*df_stderr

            results[d] = (np.array(pt_estimate[d]), CI_lower, CI_upper)
        
        
        # Categorical features a=(A1, A2), b=(B1,B2)
        
        # A1
        v = np.zeros((len(cov_matrix),))
        # individual term
        v[len(pows)] = 1
        # categorical-continuous interaction terms
        v[len(pows)+4:len(pows)+4+len(pows_int)] = np.prod(query[0:2]**pows_int, axis=1)
        # Compute std. error of estimate and confidence interval
        df_stderr = np.sqrt(v.T.dot(cov_matrix.dot(v)))
        CI_lower = pt_estimate[2][0] - z_score*df_stderr
        CI_upper = pt_estimate[2][0] + z_score*df_stderr
        results[2] = (np.array(pt_estimate[2]), np.array([CI_lower, None]), np.array([CI_upper, None]))    

        # A2
        v = np.zeros((len(cov_matrix),))
        # individual term 
        v[len(pows)+1] = 1
        # categorical-continuous interaction terms
        v[len(pows)+4+len(pows_int):len(pows)+4+2*len(pows_int)] = np.prod(query[0:2]**pows_int, axis=1)
        # Compute std. error of estimate and confidence interval
        df_stderr = np.sqrt(v.T.dot(cov_matrix.dot(v)))
        CI_lower = pt_estimate[2][1] - z_score*df_stderr
        CI_upper = pt_estimate[2][1] + z_score*df_stderr
        results[2][1][1] = CI_lower
        results[2][2][1] = CI_upper

        # B1
        v = np.zeros((len(cov_matrix),))
        # individual term 
        v[len(pows)+2] = 1
        # categorical-continuous interaction terms
        v[len(pows)+4+2*len(pows_int):len(pows)+4+3*len(pows_int)] = np.prod(query[0:2]**pows_int, axis=1)
        # Compute std. error of estimate and confidence interval
        df_stderr = np.sqrt(v.T.dot(cov_matrix.dot(v)))
        CI_lower = pt_estimate[3][0] - z_score*df_stderr
        CI_upper = pt_estimate[3][0] + z_score*df_stderr
        results[3] = (np.array(pt_estimate[3]), np.array([CI_lower, None]), np.array([CI_upper, None]))   
        
        # B2
        v = np.zeros((len(cov_matrix),))
        # individual term 
        v[len(pows)+3] = 1
        # categorical-continuous interaction terms
        v[len(pows)+4+3*len(pows_int):] = np.prod(query[0:2]**pows_int, axis=1)
        # Compute std. error of estimate and confidence interval
        df_stderr = np.sqrt(v.T.dot(cov_matrix.dot(v)))
        CI_lower = pt_estimate[3][1] - z_score*df_stderr
        CI_upper = pt_estimate[3][1] + z_score*df_stderr
        results[3][1][1] = CI_lower
        results[3][2][1] = CI_upper     
        
        return results
        
    